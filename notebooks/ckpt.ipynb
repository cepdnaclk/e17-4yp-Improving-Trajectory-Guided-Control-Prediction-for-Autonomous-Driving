{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/storage/scratch/e17-4yp-autonomous-driving/g04/TCPModels/best_model.ckpt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110040"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.perception.conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "model.perception.bn1.weight \t torch.Size([64])\n",
      "model.perception.bn1.bias \t torch.Size([64])\n",
      "model.perception.bn1.running_mean \t torch.Size([64])\n",
      "model.perception.bn1.running_var \t torch.Size([64])\n",
      "model.perception.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.0.bn1.weight \t torch.Size([64])\n",
      "model.perception.layer1.0.bn1.bias \t torch.Size([64])\n",
      "model.perception.layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.0.bn1.running_var \t torch.Size([64])\n",
      "model.perception.layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.0.bn2.weight \t torch.Size([64])\n",
      "model.perception.layer1.0.bn2.bias \t torch.Size([64])\n",
      "model.perception.layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.0.bn2.running_var \t torch.Size([64])\n",
      "model.perception.layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.1.bn1.weight \t torch.Size([64])\n",
      "model.perception.layer1.1.bn1.bias \t torch.Size([64])\n",
      "model.perception.layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.1.bn1.running_var \t torch.Size([64])\n",
      "model.perception.layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.1.bn2.weight \t torch.Size([64])\n",
      "model.perception.layer1.1.bn2.bias \t torch.Size([64])\n",
      "model.perception.layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.1.bn2.running_var \t torch.Size([64])\n",
      "model.perception.layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.2.bn1.weight \t torch.Size([64])\n",
      "model.perception.layer1.2.bn1.bias \t torch.Size([64])\n",
      "model.perception.layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.2.bn1.running_var \t torch.Size([64])\n",
      "model.perception.layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.perception.layer1.2.bn2.weight \t torch.Size([64])\n",
      "model.perception.layer1.2.bn2.bias \t torch.Size([64])\n",
      "model.perception.layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "model.perception.layer1.2.bn2.running_var \t torch.Size([64])\n",
      "model.perception.layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "model.perception.layer2.0.bn1.weight \t torch.Size([128])\n",
      "model.perception.layer2.0.bn1.bias \t torch.Size([128])\n",
      "model.perception.layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.0.bn1.running_var \t torch.Size([128])\n",
      "model.perception.layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.0.bn2.weight \t torch.Size([128])\n",
      "model.perception.layer2.0.bn2.bias \t torch.Size([128])\n",
      "model.perception.layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.0.bn2.running_var \t torch.Size([128])\n",
      "model.perception.layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "model.perception.layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "model.perception.layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "model.perception.layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "model.perception.layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.1.bn1.weight \t torch.Size([128])\n",
      "model.perception.layer2.1.bn1.bias \t torch.Size([128])\n",
      "model.perception.layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.1.bn1.running_var \t torch.Size([128])\n",
      "model.perception.layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.1.bn2.weight \t torch.Size([128])\n",
      "model.perception.layer2.1.bn2.bias \t torch.Size([128])\n",
      "model.perception.layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.1.bn2.running_var \t torch.Size([128])\n",
      "model.perception.layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.2.bn1.weight \t torch.Size([128])\n",
      "model.perception.layer2.2.bn1.bias \t torch.Size([128])\n",
      "model.perception.layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.2.bn1.running_var \t torch.Size([128])\n",
      "model.perception.layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.2.bn2.weight \t torch.Size([128])\n",
      "model.perception.layer2.2.bn2.bias \t torch.Size([128])\n",
      "model.perception.layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.2.bn2.running_var \t torch.Size([128])\n",
      "model.perception.layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.3.bn1.weight \t torch.Size([128])\n",
      "model.perception.layer2.3.bn1.bias \t torch.Size([128])\n",
      "model.perception.layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.3.bn1.running_var \t torch.Size([128])\n",
      "model.perception.layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.perception.layer2.3.bn2.weight \t torch.Size([128])\n",
      "model.perception.layer2.3.bn2.bias \t torch.Size([128])\n",
      "model.perception.layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "model.perception.layer2.3.bn2.running_var \t torch.Size([128])\n",
      "model.perception.layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "model.perception.layer3.0.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.0.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.0.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.0.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.0.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.0.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "model.perception.layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "model.perception.layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "model.perception.layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.1.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.1.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.1.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.1.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.1.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.1.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.2.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.2.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.2.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.2.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.2.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.2.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.3.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.3.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.3.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.3.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.3.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.3.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.4.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.4.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.4.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.4.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.4.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.4.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.5.bn1.weight \t torch.Size([256])\n",
      "model.perception.layer3.5.bn1.bias \t torch.Size([256])\n",
      "model.perception.layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.5.bn1.running_var \t torch.Size([256])\n",
      "model.perception.layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.perception.layer3.5.bn2.weight \t torch.Size([256])\n",
      "model.perception.layer3.5.bn2.bias \t torch.Size([256])\n",
      "model.perception.layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "model.perception.layer3.5.bn2.running_var \t torch.Size([256])\n",
      "model.perception.layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "model.perception.layer4.0.bn1.weight \t torch.Size([512])\n",
      "model.perception.layer4.0.bn1.bias \t torch.Size([512])\n",
      "model.perception.layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.0.bn1.running_var \t torch.Size([512])\n",
      "model.perception.layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.perception.layer4.0.bn2.weight \t torch.Size([512])\n",
      "model.perception.layer4.0.bn2.bias \t torch.Size([512])\n",
      "model.perception.layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.0.bn2.running_var \t torch.Size([512])\n",
      "model.perception.layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "model.perception.layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "model.perception.layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "model.perception.layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "model.perception.layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.perception.layer4.1.bn1.weight \t torch.Size([512])\n",
      "model.perception.layer4.1.bn1.bias \t torch.Size([512])\n",
      "model.perception.layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.1.bn1.running_var \t torch.Size([512])\n",
      "model.perception.layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.perception.layer4.1.bn2.weight \t torch.Size([512])\n",
      "model.perception.layer4.1.bn2.bias \t torch.Size([512])\n",
      "model.perception.layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.1.bn2.running_var \t torch.Size([512])\n",
      "model.perception.layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.perception.layer4.2.bn1.weight \t torch.Size([512])\n",
      "model.perception.layer4.2.bn1.bias \t torch.Size([512])\n",
      "model.perception.layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.2.bn1.running_var \t torch.Size([512])\n",
      "model.perception.layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.perception.layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.perception.layer4.2.bn2.weight \t torch.Size([512])\n",
      "model.perception.layer4.2.bn2.bias \t torch.Size([512])\n",
      "model.perception.layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "model.perception.layer4.2.bn2.running_var \t torch.Size([512])\n",
      "model.perception.layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.perception.fc.weight \t torch.Size([1000, 512])\n",
      "model.perception.fc.bias \t torch.Size([1000])\n",
      "model.measurements.0.weight \t torch.Size([128, 9])\n",
      "model.measurements.0.bias \t torch.Size([128])\n",
      "model.measurements.2.weight \t torch.Size([128, 128])\n",
      "model.measurements.2.bias \t torch.Size([128])\n",
      "model.join_traj.0.weight \t torch.Size([512, 1128])\n",
      "model.join_traj.0.bias \t torch.Size([512])\n",
      "model.join_traj.2.weight \t torch.Size([512, 512])\n",
      "model.join_traj.2.bias \t torch.Size([512])\n",
      "model.join_traj.4.weight \t torch.Size([256, 512])\n",
      "model.join_traj.4.bias \t torch.Size([256])\n",
      "model.join_ctrl.0.weight \t torch.Size([512, 640])\n",
      "model.join_ctrl.0.bias \t torch.Size([512])\n",
      "model.join_ctrl.2.weight \t torch.Size([512, 512])\n",
      "model.join_ctrl.2.bias \t torch.Size([512])\n",
      "model.join_ctrl.4.weight \t torch.Size([256, 512])\n",
      "model.join_ctrl.4.bias \t torch.Size([256])\n",
      "model.speed_branch.0.weight \t torch.Size([256, 1000])\n",
      "model.speed_branch.0.bias \t torch.Size([256])\n",
      "model.speed_branch.2.weight \t torch.Size([256, 256])\n",
      "model.speed_branch.2.bias \t torch.Size([256])\n",
      "model.speed_branch.5.weight \t torch.Size([1, 256])\n",
      "model.speed_branch.5.bias \t torch.Size([1])\n",
      "model.value_branch_traj.0.weight \t torch.Size([256, 256])\n",
      "model.value_branch_traj.0.bias \t torch.Size([256])\n",
      "model.value_branch_traj.2.weight \t torch.Size([256, 256])\n",
      "model.value_branch_traj.2.bias \t torch.Size([256])\n",
      "model.value_branch_traj.5.weight \t torch.Size([1, 256])\n",
      "model.value_branch_traj.5.bias \t torch.Size([1])\n",
      "model.value_branch_ctrl.0.weight \t torch.Size([256, 256])\n",
      "model.value_branch_ctrl.0.bias \t torch.Size([256])\n",
      "model.value_branch_ctrl.2.weight \t torch.Size([256, 256])\n",
      "model.value_branch_ctrl.2.bias \t torch.Size([256])\n",
      "model.value_branch_ctrl.5.weight \t torch.Size([1, 256])\n",
      "model.value_branch_ctrl.5.bias \t torch.Size([1])\n",
      "model.policy_head.0.weight \t torch.Size([256, 256])\n",
      "model.policy_head.0.bias \t torch.Size([256])\n",
      "model.policy_head.2.weight \t torch.Size([256, 256])\n",
      "model.policy_head.2.bias \t torch.Size([256])\n",
      "model.decoder_ctrl.weight_ih \t torch.Size([768, 260])\n",
      "model.decoder_ctrl.weight_hh \t torch.Size([768, 256])\n",
      "model.decoder_ctrl.bias_ih \t torch.Size([768])\n",
      "model.decoder_ctrl.bias_hh \t torch.Size([768])\n",
      "model.output_ctrl.0.weight \t torch.Size([256, 256])\n",
      "model.output_ctrl.0.bias \t torch.Size([256])\n",
      "model.output_ctrl.2.weight \t torch.Size([256, 256])\n",
      "model.output_ctrl.2.bias \t torch.Size([256])\n",
      "model.dist_mu.0.weight \t torch.Size([2, 256])\n",
      "model.dist_mu.0.bias \t torch.Size([2])\n",
      "model.dist_sigma.0.weight \t torch.Size([2, 256])\n",
      "model.dist_sigma.0.bias \t torch.Size([2])\n",
      "model.decoder_traj.weight_ih \t torch.Size([768, 4])\n",
      "model.decoder_traj.weight_hh \t torch.Size([768, 256])\n",
      "model.decoder_traj.bias_ih \t torch.Size([768])\n",
      "model.decoder_traj.bias_hh \t torch.Size([768])\n",
      "model.output_traj.weight \t torch.Size([2, 256])\n",
      "model.output_traj.bias \t torch.Size([2])\n",
      "model.init_att.0.weight \t torch.Size([256, 128])\n",
      "model.init_att.0.bias \t torch.Size([256])\n",
      "model.init_att.2.weight \t torch.Size([232, 256])\n",
      "model.init_att.2.bias \t torch.Size([232])\n",
      "model.wp_att.0.weight \t torch.Size([256, 512])\n",
      "model.wp_att.0.bias \t torch.Size([256])\n",
      "model.wp_att.2.weight \t torch.Size([232, 256])\n",
      "model.wp_att.2.bias \t torch.Size([232])\n",
      "model.merge.0.weight \t torch.Size([512, 768])\n",
      "model.merge.0.bias \t torch.Size([512])\n",
      "model.merge.2.weight \t torch.Size([256, 512])\n",
      "model.merge.2.bias \t torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in checkpoint['state_dict']:\n",
    "    print(param_tensor, \"\\t\", checkpoint['state_dict'][param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint: {'monitor': 'val_loss',\n",
       "  'best_model_score': tensor(0.4388),\n",
       "  'best_model_path': '/home/kin/TCP/log/TCP/best_epoch=39-val_loss=0.439.ckpt',\n",
       "  'current_score': tensor(0.4657),\n",
       "  'dirpath': '/home/kin/TCP/log/TCP'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['callbacks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'step_size': 30,\n",
       "  'gamma': 0.5,\n",
       "  'base_lrs': [0.0001],\n",
       "  'last_epoch': 6,\n",
       "  '_step_count': 7,\n",
       "  'verbose': False,\n",
       "  '_get_lr_called_within_step': False,\n",
       "  '_last_lr': [0.0001]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['lr_schedulers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_state_dict = torch.load('/storage/scratch/e17-4yp-autonomous-driving/g04/TCP/roach/log/ckpt_11833344.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['policy_state_dict', 'policy_init_kwargs', 'train_init_kwargs'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rl_state_dict['policy_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_extractor.cnn.0.weight \t torch.Size([8, 15, 5, 5])\n",
      "features_extractor.cnn.0.bias \t torch.Size([8])\n",
      "features_extractor.cnn.2.weight \t torch.Size([16, 8, 5, 5])\n",
      "features_extractor.cnn.2.bias \t torch.Size([16])\n",
      "features_extractor.cnn.4.weight \t torch.Size([32, 16, 5, 5])\n",
      "features_extractor.cnn.4.bias \t torch.Size([32])\n",
      "features_extractor.cnn.6.weight \t torch.Size([64, 32, 3, 3])\n",
      "features_extractor.cnn.6.bias \t torch.Size([64])\n",
      "features_extractor.cnn.8.weight \t torch.Size([128, 64, 3, 3])\n",
      "features_extractor.cnn.8.bias \t torch.Size([128])\n",
      "features_extractor.cnn.10.weight \t torch.Size([256, 128, 3, 3])\n",
      "features_extractor.cnn.10.bias \t torch.Size([256])\n",
      "features_extractor.linear.0.weight \t torch.Size([512, 1280])\n",
      "features_extractor.linear.0.bias \t torch.Size([512])\n",
      "features_extractor.linear.2.weight \t torch.Size([256, 512])\n",
      "features_extractor.linear.2.bias \t torch.Size([256])\n",
      "features_extractor.state_linear.0.weight \t torch.Size([256, 6])\n",
      "features_extractor.state_linear.0.bias \t torch.Size([256])\n",
      "features_extractor.state_linear.2.weight \t torch.Size([256, 256])\n",
      "features_extractor.state_linear.2.bias \t torch.Size([256])\n",
      "policy_head.0.weight \t torch.Size([256, 256])\n",
      "policy_head.0.bias \t torch.Size([256])\n",
      "policy_head.2.weight \t torch.Size([256, 256])\n",
      "policy_head.2.bias \t torch.Size([256])\n",
      "dist_mu.0.weight \t torch.Size([2, 256])\n",
      "dist_mu.0.bias \t torch.Size([2])\n",
      "dist_sigma.0.weight \t torch.Size([2, 256])\n",
      "dist_sigma.0.bias \t torch.Size([2])\n",
      "value_head.0.weight \t torch.Size([256, 256])\n",
      "value_head.0.bias \t torch.Size([256])\n",
      "value_head.2.weight \t torch.Size([256, 256])\n",
      "value_head.2.bias \t torch.Size([256])\n",
      "value_head.4.weight \t torch.Size([1, 256])\n",
      "value_head.4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in rl_state_dict['policy_state_dict']:\n",
    "    print(param_tensor, \"\\t\", rl_state_dict['policy_state_dict'][param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'value_head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dist_mu.0.weight', 'dist_mu.0.bias']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = rl_state_dict['policy_state_dict']\n",
    "rl_keys = [k for k in policy.keys() if key in k]\n",
    "rl_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_branch_traj = nn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(256, 256),\n",
    "\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\tnn.Linear(256, 256),\n",
    "\t\t\t\t\tnn.Dropout2d(p=0.5),\n",
    "\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\tnn.Linear(256, 1),\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias', '5.weight', '5.bias'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il_keys = value_branch_traj.state_dict().keys()\n",
    "assert len(rl_keys) == len(value_branch_traj.state_dict().keys()), f'mismatch number of layers loading {key}'\n",
    "new_state_dict = OrderedDict()\n",
    "for k_il, k_rl in zip(il_keys, rl_keys):\n",
    "    new_state_dict[k_il] = policy[k_rl]\n",
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint15 = torch.load('/storage/scratch/e17-4yp-autonomous-driving/g04/TCP/log/TCP/epoch=15-last.ckpt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint15.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/storage/scratch/e17-4yp-autonomous-driving/g04/TCPDataset/tcp_carla_data/town01/routes_town01_03_28_17_13_31/supervision/0003.npy',\n",
    "               allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_command']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
